- **Sklearn 라이브러리 주요 모듈**
    
    
    | 분류 | 모듈 | 설명 |
    | --- | --- | --- |
    | 변수 처리 | sklearn.preprocessing | 전처리에 필요한 기능 제공 (인코딩, 정규화, 스케일링 등) |
    |  | sklearn.feature_seletion | 변수 선택에 필요한 기능 제공 |
    |  | sklearn.feature_extraction | 텍스트, 이미지 데이터 같이 벡터화된 변수를 추출하는데 사용됨 |
    | 변수 처리 & 차원 축소 | sklearn.decomposition | 차원축소와 관련된 알고리즘 제공
    PCA, NMF, Truncated SVD를 통해 차원 축소 기능 수행 |
    | 데이터 분리/검증 & 매개변수 튜닝 | sklearn.model_selection | 데이터 분할 (train_test_split) 함수 내장
    최적의 하이퍼파라미터를 찾는 기능 제공 |
    | 평가 | sklearn.metrics  | 분류, 회귀, 클러스터링 등에 대한 다양한 성능 측정 방법을 제공 |
    | 머신러닝 알고리즘 (모델) | sklearn.ensemble | 앙상블 알고리즘 제공
    랜덤포레스트, 에이다 부스트, 그래디언트 부스팅 등을 제공 |
    |  | sklearn.linear_model | 선형회귀, 릿지(Ridge), 라쏘(Lasso) 및 로지스틱 회귀 등 회귀 관련 알고리즘 지원
    SGD관련 알고리즘도 지원 |
    |  | sklearn.naive_bayes | 나이브 베이즈 알고리즘 제공
    가우시안 NB, 다항분포 NB 등이 있음 |
    |  | sklearn.neighbors | 최근접 이웃 알고리즘 (KNN) 알고리즘 제공 |
    |  | sklearn.svm | SVM 알고리즘 제공 |
    |  | sklearn.tree | 트리기반 머신러닝 알고리즘 제공
    의사결정 트리 알고리즘 등이 있음 |
    |  | sklearn.cluster | 비지도 클러스터링 알고리즘을 제공함
    K-means, 계층형, DBSCAN 등이 있음 |
    | 유틸리티 | sklearn.pipeline | - |
- **Pandas 라이브러리 주요 모듈**
    
    pass
    

1. **데이터 읽어오기**
    
    실습 환경에서 제공하는 데이터는 X_train, Y_train, X_test 임.
    
    데이터 형식에 따라 컬럼을 구분해놓으면 좋다.
    
    ```python
    df.info()
    # print
    '''
    Dtype = float64, int64 : numerical features
    Dtype = object : categorical features
    '''
    
    **COL_DEL = ['name'] # 삭제할 컬럼?
    COL_NUM = ['mpg', 'cylinders', 'displacement', 'horsepower', 'weight']
    COL_CAT = ['class', 'type', 'sex']
    COL_Y = ['isUSA']**
    ```
    
2. **데이터 분할 :** X_train, Y_train 데이터 train, valid, test set으로 분리하기
    
    ```python
    **from sklearn.model_selection import train_test_split**
    
    X_train, X_valid, Y_train, Y_valid = train_test_split(X_train, Y_train, test_size=0.3)
    ```
    
3. **데이터 전처리 :** 범주형 데이터 - 원핫인코딩 or 더미변수화 / 수치형 데이터 - 스케일러
    
    <aside>
    💡 수치형, 범주형 데이터 변환
    
    - **수치형 데이터**의 경우 scaler를 **trainset**으로 fit 한 후 transform함수를 사용하여 train, valid, test 데이터에대한 스케일링을 수행한다.
    
    ```python
    # scaling
    **from sklearn.preprocessing import StandardScaler
    from sklearn.preprocessing import MinMaxScaler**
    
    scaler = StandardScaler()
    scaler.fit(X_train[COL_NUM])
    
    X_train[COL_NUM] = scaler.transform(X_train[COL_NUM])
    X_valid[COL_NUM] = scaler.transform(X_valid[COL_NUM])
    X_test[COL_NUM] = scaler.transform(X_test[COL_NUM])
    ```
    
    또한 수치형 데이터의 경우 분위수 등을 확인하여 범주형 변수로 변환해줄 수도 있다.
    
    ```python
    # **심화**
    df.quantile()
    ```
    
    - **범주형 데이터**의 경우 encoder를 **trainset, testset으로 fit**한 후 transform함수를 사용하여 train, valid, test 데이터에 대해 인코딩을 수행한다.
    - 또한 원핫 인코딩을 적용하였을 때 **numpy 행렬 형식의 희소 행렬을 반환**하기 때문에 데이터프레임으로 변경해주는 작업이 필요하다.
    
    ```python
    # encoding
    **from sklearn.preprocessing import OneHotEncoder**
    
    X = pd.concat([X_train, X_test])
    ohe = OneHotEncoder(handle_unknown='ignore')
    ohe.fit(X[COL_CAT])
    
    X_train_res = ohe.transform(X_train[COL_CAT])
    X_test_res = ohe.transform(X_test[COL_CAT])
    
    # convert to dataframe
    X_train_ohe = pd.Dataframe(X_train_res.todense(), columns = ohe.get_feature_names())
    X_test_ohe = pd.Dataframe(X_test_res.todense(), columns = ohe.get_feature_names())
    
    X_train_fin = pd.concat([X_train[COL_NUM], X_train_ohe], axis=1)
    X_test_fin = pd.concat([X_test[COL_NUM], X_test_ohe], axis=1)
    ```
    
    - Pandas로 get_dummies 함수를 사용하여 구현할 수 있다.
    
    ```python
    X = pd.concat([X_train, X_test])
    # 범주형 변수에 원핫 인코딩된 데이터를 추가함
    one_hot_df = pd.get_dummies(df, prefix='class', columns=['class'], drop_first=False)
    # drop_first를 True로 setting하면 dummy 변수로 변환할 수 있다.
    # 원핫인코딩과 더미변수화의 차이 - 더미변수의 경우 기준 카테고리를 드랍함
    for _col in COL_CAT:
    	one_hot_df = pd.get_dummies(df, prefix=_col, columns=[_col], drop_first=False)
    ```
    
    - 범주형 데이터 중 **카디날리티가 높은 변수**의 경우 **레이블 인코딩**을 적용할 수도 있다.
        - cardinality: 한 변수가 가질 수 있는 고유한 값 (unique값)의 개수
        - *레이블 인코딩을 한 데이터는 선형 모델에 적합하지 않으므로, 비선형 모델인 랜덤 포레스트나 XGBoost 알고리즘을 사용하도록 한다.*
    
    ```python
    **# 심화**
    **from sklearn.preprocessing import LabelEncoder**
    
    X = pd.concat([X_train, X_test])
    
    for _col in COL_CAT:
    	le = LabelEncoder()
    	le.fit(X_train[_col])
    	X_train[_col] = le.tranform(X_train[_col])
    	X_test[_col] = le.tranform(X_test[_col])
    ```
    
    </aside>
    
    <aside>
    💡 b. 결측값 처리
    
    결측값은 info() 함수로 확인할 수 있다.
    
    ```python
    	# 결측값 처리 코드
    ```
    
    </aside>
    
    <aside>
    💡 c. 이상값 처리
    
    *이상 값은 분위수 등을 활용하여 처리할 수 있을것 같다* 
    
    </aside>
    

1. **데이터 모형 구축**
    
    model을 학습시키기 위해 먼저 모델을 구축한 후 전처리를 진행한 데이터로 fit 함으로써 학습을 진행한다.
    
    모델은 task 종류에 따라 회귀모델, 분류모델로 구분한다.
    
    **회귀모델** 
    
    선형회귀(LinearRegression), 
    
    **분류모델**
    
    랜덤포레스트(RandomForestClassifier)
    
    ```python
    # Random Forest classifier
    **from sklearn.ensemble import RandomForestClassifier**
    
    # basic version
    model_rf = RandomForestClassifier()
    model_rf.fit(X_train, y_train.values.ravel())
    
    # feature importance 조회 가능
    feature_impt = model_rf.feature_importances_
    ```
    
    ```python
    # XGBoost classifier
    **from xgboost import XGBClassifier**
    
    # basic version
    model_xgb1 = XGBClassifier() 
    model_xgb1.fit(X_train, Y_train.value.ravel())
    
    # custom version
    model_xgb2 = XGBClassifier(n_estimators=1000, learning_rate=0.1, max_depth=10)
    model_xg2.fit(X_train, Y_train.values.ravel(), **early_stopping_rounds=50,** \
    							**eval_metric='auc', eval_set=[(X_valid, Y_valid)]**, verbose=10)
    # 학습시 validation set 사용 가능
    # eval_metric 적용 가능
    # feature importance 조회 가능
    feature_impt = model_xgb2.feature_importances_
    ```
    
    ```python
    # SVM classifier
    ```
    
    ```python
    # lightgbm
    
    ```
    

1. **모델 튜닝**
    1. **특징 중요도** 확인을 통해 **특징 삭제**
        
        ** 학습된 모델을 통해 특징중요도(feature importance)를 확인 가능한 모델들이 있으며, 이를 통해 중요도가 낮은 특징을 삭제해주는 등 모델 최적화에 활용 가능함.*
        
        [https://ahnty0122.tistory.com/51](https://ahnty0122.tistory.com/51)
        
        ![image](https://user-images.githubusercontent.com/86610517/181142202-809bc47e-7a11-4a8f-85a5-cf69e38bae23.png)
        
        ```python
        # X_train과 y_train이 각각 나왔을 때 join을 통해 특징 중요도 확인
        df.join(other.set_index('key'), on='key')
        ```
        
        ![image](https://user-images.githubusercontent.com/86610517/181142216-c9646787-31db-4db3-9345-4ceae4fe2b2b.png)
        
        코드 실행 후
        
        ```python
        df[df.columns[1:]].corr()['target'].sort_values(ascending=False)
        '''
        Output:
        Chance of Admit      1.000000
        CGPA                 0.869994
        GRE Score            0.799788
        TOEFL Score          0.789996
        University Rating    0.690693
        SOP                  0.688587
        LOR                  0.647938
        Research             0.574697
        Name: Chance of Admit , dtype: float64
        '''
        ```
        
    2. ~~모델의 **하이퍼파라미터** 튜닝~~
        
        *for문을 통해 후보 파라미터들을 설정하여 best model을 찾을수도 있음
        
        ```python
        **# 심화**
        from sklearn.model_selection import GridSearchCV
        #page 252
        
        ```
        
    3. 
2. **데이터 모형 평가**
    
    평가형 데이터로 예측값을 구한다.
    
    - predict 함수를 사용하면 threshold가 0.5인 상태로 예측값을 구하게 된다.
    - predict_proba 함수를 사용하면 예측 확률을 구할 수 있다.
    
    **분류 모형 평가**
    
    ```python
    # classification_report 함수를 사용하면 평가지표를 한번에 다 구할 수 있어서 좋다.
    **from sklearn.metrics import classification_report**
    
    Y_pred = model.predict(X_test_preprocessed)
    print(classification_report(Y_test, Y_pred, labels=[0, 1])
    '''
    **output:**
    							presision, recall, f1-score, support
    label 1
    label 2
    label ..
    ..
    
    accuracy
    macro avg
    weighted avg
    '''
    
    # AUROC 값을 구하기 위해서는 predict_proba 함수를 사용해야한다.
    from sklearn.metrics import roc_auc_score
    
    # 추출해야하는 확률값이 두번째 열에 있을 때 [:, 1]을 붙여주어야한다.
    Y_pred_prob = model.predict_proba(X_test_preprocessed)[:, 1]
    print(roc_auc_score(Y_test, Y_pred_prob) 
    ```
    
    **회귀 모형 평가**
    
    ```python
    **from sklearn.metrics import mean_squared_error, r2_score**
    
    mse = mean_squared_error(Y_valid, Y_valid_pred)
    rmse = mean_squared_error(Y_valid, Y_valid_pred, squared=False)
    
    ```
    
    **저장 코드**
    
    ```python
    pd.DataFrame({'id': X_test['Serial No.'], 'target': pred}).to_csv('003000000.csv', index=False)
    ```

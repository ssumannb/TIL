- **Sklearn ë¼ì´ë¸ŒëŸ¬ë¦¬ ì£¼ìš” ëª¨ë“ˆ**
    
    
    | ë¶„ë¥˜ | ëª¨ë“ˆ | ì„¤ëª… |
    | --- | --- | --- |
    | ë³€ìˆ˜ ì²˜ë¦¬ | sklearn.preprocessing | ì „ì²˜ë¦¬ì— í•„ìš”í•œ ê¸°ëŠ¥ ì œê³µ (ì¸ì½”ë”©, ì •ê·œí™”, ìŠ¤ì¼€ì¼ë§ ë“±) |
    |  | sklearn.feature_seletion | ë³€ìˆ˜ ì„ íƒì— í•„ìš”í•œ ê¸°ëŠ¥ ì œê³µ |
    |  | sklearn.feature_extraction | í…ìŠ¤íŠ¸, ì´ë¯¸ì§€ ë°ì´í„° ê°™ì´ ë²¡í„°í™”ëœ ë³€ìˆ˜ë¥¼ ì¶”ì¶œí•˜ëŠ”ë° ì‚¬ìš©ë¨ |
    | ë³€ìˆ˜ ì²˜ë¦¬ & ì°¨ì› ì¶•ì†Œ | sklearn.decomposition | ì°¨ì›ì¶•ì†Œì™€ ê´€ë ¨ëœ ì•Œê³ ë¦¬ì¦˜ ì œê³µ
    PCA, NMF, Truncated SVDë¥¼ í†µí•´ ì°¨ì› ì¶•ì†Œ ê¸°ëŠ¥ ìˆ˜í–‰ |
    | ë°ì´í„° ë¶„ë¦¬/ê²€ì¦ & ë§¤ê°œë³€ìˆ˜ íŠœë‹ | sklearn.model_selection | ë°ì´í„° ë¶„í•  (train_test_split) í•¨ìˆ˜ ë‚´ì¥
    ìµœì ì˜ í•˜ì´í¼íŒŒë¼ë¯¸í„°ë¥¼ ì°¾ëŠ” ê¸°ëŠ¥ ì œê³µ |
    | í‰ê°€ | sklearn.metrics  | ë¶„ë¥˜, íšŒê·€, í´ëŸ¬ìŠ¤í„°ë§ ë“±ì— ëŒ€í•œ ë‹¤ì–‘í•œ ì„±ëŠ¥ ì¸¡ì • ë°©ë²•ì„ ì œê³µ |
    | ë¨¸ì‹ ëŸ¬ë‹ ì•Œê³ ë¦¬ì¦˜ (ëª¨ë¸) | sklearn.ensemble | ì•™ìƒë¸” ì•Œê³ ë¦¬ì¦˜ ì œê³µ
    ëœë¤í¬ë ˆìŠ¤íŠ¸, ì—ì´ë‹¤ ë¶€ìŠ¤íŠ¸, ê·¸ë˜ë””ì–¸íŠ¸ ë¶€ìŠ¤íŒ… ë“±ì„ ì œê³µ |
    |  | sklearn.linear_model | ì„ í˜•íšŒê·€, ë¦¿ì§€(Ridge), ë¼ì˜(Lasso) ë° ë¡œì§€ìŠ¤í‹± íšŒê·€ ë“± íšŒê·€ ê´€ë ¨ ì•Œê³ ë¦¬ì¦˜ ì§€ì›
    SGDê´€ë ¨ ì•Œê³ ë¦¬ì¦˜ë„ ì§€ì› |
    |  | sklearn.naive_bayes | ë‚˜ì´ë¸Œ ë² ì´ì¦ˆ ì•Œê³ ë¦¬ì¦˜ ì œê³µ
    ê°€ìš°ì‹œì•ˆ NB, ë‹¤í•­ë¶„í¬ NB ë“±ì´ ìˆìŒ |
    |  | sklearn.neighbors | ìµœê·¼ì ‘ ì´ì›ƒ ì•Œê³ ë¦¬ì¦˜ (KNN) ì•Œê³ ë¦¬ì¦˜ ì œê³µ |
    |  | sklearn.svm | SVM ì•Œê³ ë¦¬ì¦˜ ì œê³µ |
    |  | sklearn.tree | íŠ¸ë¦¬ê¸°ë°˜ ë¨¸ì‹ ëŸ¬ë‹ ì•Œê³ ë¦¬ì¦˜ ì œê³µ
    ì˜ì‚¬ê²°ì • íŠ¸ë¦¬ ì•Œê³ ë¦¬ì¦˜ ë“±ì´ ìˆìŒ |
    |  | sklearn.cluster | ë¹„ì§€ë„ í´ëŸ¬ìŠ¤í„°ë§ ì•Œê³ ë¦¬ì¦˜ì„ ì œê³µí•¨
    K-means, ê³„ì¸µí˜•, DBSCAN ë“±ì´ ìˆìŒ |
    | ìœ í‹¸ë¦¬í‹° | sklearn.pipeline | - |
- **Pandas ë¼ì´ë¸ŒëŸ¬ë¦¬ ì£¼ìš” ëª¨ë“ˆ**
    
    pass
    

1. **ë°ì´í„° ì½ì–´ì˜¤ê¸°**
    
    ì‹¤ìŠµ í™˜ê²½ì—ì„œ ì œê³µí•˜ëŠ” ë°ì´í„°ëŠ” X_train, Y_train, X_test ì„.
    
    ë°ì´í„° í˜•ì‹ì— ë”°ë¼ ì»¬ëŸ¼ì„ êµ¬ë¶„í•´ë†“ìœ¼ë©´ ì¢‹ë‹¤.
    
    ```python
    df.info()
    # print
    '''
    Dtype = float64, int64 : numerical features
    Dtype = object : categorical features
    '''
    
    **COL_DEL = ['name'] # ì‚­ì œí•  ì»¬ëŸ¼?
    COL_NUM = ['mpg', 'cylinders', 'displacement', 'horsepower', 'weight']
    COL_CAT = ['class', 'type', 'sex']
    COL_Y = ['isUSA']**
    ```
    
2. **ë°ì´í„° ë¶„í•  :** X_train, Y_train ë°ì´í„° train, valid, test setìœ¼ë¡œ ë¶„ë¦¬í•˜ê¸°
    
    ```python
    **from sklearn.model_selection import train_test_split**
    
    X_train, X_valid, Y_train, Y_valid = train_test_split(X_train, Y_train, test_size=0.3)
    ```
    
3. **ë°ì´í„° ì „ì²˜ë¦¬ :** ë²”ì£¼í˜• ë°ì´í„° - ì›í•«ì¸ì½”ë”© or ë”ë¯¸ë³€ìˆ˜í™” / ìˆ˜ì¹˜í˜• ë°ì´í„° - ìŠ¤ì¼€ì¼ëŸ¬
    
    <aside>
    ğŸ’¡ ìˆ˜ì¹˜í˜•, ë²”ì£¼í˜• ë°ì´í„° ë³€í™˜
    
    - **ìˆ˜ì¹˜í˜• ë°ì´í„°**ì˜ ê²½ìš° scalerë¥¼ **trainset**ìœ¼ë¡œ fit í•œ í›„ transformí•¨ìˆ˜ë¥¼ ì‚¬ìš©í•˜ì—¬ train, valid, test ë°ì´í„°ì—ëŒ€í•œ ìŠ¤ì¼€ì¼ë§ì„ ìˆ˜í–‰í•œë‹¤.
    
    ```python
    # scaling
    **from sklearn.preprocessing import StandardScaler
    from sklearn.preprocessing import MinMaxScaler**
    
    scaler = StandardScaler()
    scaler.fit(X_train[COL_NUM])
    
    X_train[COL_NUM] = scaler.transform(X_train[COL_NUM])
    X_valid[COL_NUM] = scaler.transform(X_valid[COL_NUM])
    X_test[COL_NUM] = scaler.transform(X_test[COL_NUM])
    ```
    
    ë˜í•œ ìˆ˜ì¹˜í˜• ë°ì´í„°ì˜ ê²½ìš° ë¶„ìœ„ìˆ˜ ë“±ì„ í™•ì¸í•˜ì—¬ ë²”ì£¼í˜• ë³€ìˆ˜ë¡œ ë³€í™˜í•´ì¤„ ìˆ˜ë„ ìˆë‹¤.
    
    ```python
    # **ì‹¬í™”**
    df.quantile()
    ```
    
    - **ë²”ì£¼í˜• ë°ì´í„°**ì˜ ê²½ìš° encoderë¥¼ **trainset, testsetìœ¼ë¡œ fit**í•œ í›„ transformí•¨ìˆ˜ë¥¼ ì‚¬ìš©í•˜ì—¬ train, valid, test ë°ì´í„°ì— ëŒ€í•´ ì¸ì½”ë”©ì„ ìˆ˜í–‰í•œë‹¤.
    - ë˜í•œ ì›í•« ì¸ì½”ë”©ì„ ì ìš©í•˜ì˜€ì„ ë•Œ **numpy í–‰ë ¬ í˜•ì‹ì˜ í¬ì†Œ í–‰ë ¬ì„ ë°˜í™˜**í•˜ê¸° ë•Œë¬¸ì— ë°ì´í„°í”„ë ˆì„ìœ¼ë¡œ ë³€ê²½í•´ì£¼ëŠ” ì‘ì—…ì´ í•„ìš”í•˜ë‹¤.
    
    ```python
    # encoding
    **from sklearn.preprocessing import OneHotEncoder**
    
    X = pd.concat([X_train, X_test])
    ohe = OneHotEncoder(handle_unknown='ignore')
    ohe.fit(X[COL_CAT])
    
    X_train_res = ohe.transform(X_train[COL_CAT])
    X_test_res = ohe.transform(X_test[COL_CAT])
    
    # convert to dataframe
    X_train_ohe = pd.Dataframe(X_train_res.todense(), columns = ohe.get_feature_names())
    X_test_ohe = pd.Dataframe(X_test_res.todense(), columns = ohe.get_feature_names())
    
    X_train_fin = pd.concat([X_train[COL_NUM], X_train_ohe], axis=1)
    X_test_fin = pd.concat([X_test[COL_NUM], X_test_ohe], axis=1)
    ```
    
    - Pandasë¡œ get_dummies í•¨ìˆ˜ë¥¼ ì‚¬ìš©í•˜ì—¬ êµ¬í˜„í•  ìˆ˜ ìˆë‹¤.
    
    ```python
    X = pd.concat([X_train, X_test])
    # ë²”ì£¼í˜• ë³€ìˆ˜ì— ì›í•« ì¸ì½”ë”©ëœ ë°ì´í„°ë¥¼ ì¶”ê°€í•¨
    one_hot_df = pd.get_dummies(df, prefix='class', columns=['class'], drop_first=False)
    # drop_firstë¥¼ Trueë¡œ settingí•˜ë©´ dummy ë³€ìˆ˜ë¡œ ë³€í™˜í•  ìˆ˜ ìˆë‹¤.
    # ì›í•«ì¸ì½”ë”©ê³¼ ë”ë¯¸ë³€ìˆ˜í™”ì˜ ì°¨ì´ - ë”ë¯¸ë³€ìˆ˜ì˜ ê²½ìš° ê¸°ì¤€ ì¹´í…Œê³ ë¦¬ë¥¼ ë“œëí•¨
    for _col in COL_CAT:
    	one_hot_df = pd.get_dummies(df, prefix=_col, columns=[_col], drop_first=False)
    ```
    
    - ë²”ì£¼í˜• ë°ì´í„° ì¤‘ **ì¹´ë””ë‚ ë¦¬í‹°ê°€ ë†’ì€ ë³€ìˆ˜**ì˜ ê²½ìš° **ë ˆì´ë¸” ì¸ì½”ë”©**ì„ ì ìš©í•  ìˆ˜ë„ ìˆë‹¤.
        - cardinality: í•œ ë³€ìˆ˜ê°€ ê°€ì§ˆ ìˆ˜ ìˆëŠ” ê³ ìœ í•œ ê°’ (uniqueê°’)ì˜ ê°œìˆ˜
        - *ë ˆì´ë¸” ì¸ì½”ë”©ì„ í•œ ë°ì´í„°ëŠ” ì„ í˜• ëª¨ë¸ì— ì í•©í•˜ì§€ ì•Šìœ¼ë¯€ë¡œ, ë¹„ì„ í˜• ëª¨ë¸ì¸ ëœë¤ í¬ë ˆìŠ¤íŠ¸ë‚˜ XGBoost ì•Œê³ ë¦¬ì¦˜ì„ ì‚¬ìš©í•˜ë„ë¡ í•œë‹¤.*
    
    ```python
    **# ì‹¬í™”**
    **from sklearn.preprocessing import LabelEncoder**
    
    X = pd.concat([X_train, X_test])
    
    for _col in COL_CAT:
    	le = LabelEncoder()
    	le.fit(X_train[_col])
    	X_train[_col] = le.tranform(X_train[_col])
    	X_test[_col] = le.tranform(X_test[_col])
    ```
    
    </aside>
    
    <aside>
    ğŸ’¡ b. ê²°ì¸¡ê°’ ì²˜ë¦¬
    
    ê²°ì¸¡ê°’ì€ info() í•¨ìˆ˜ë¡œ í™•ì¸í•  ìˆ˜ ìˆë‹¤.
    
    ```python
    	# ê²°ì¸¡ê°’ ì²˜ë¦¬ ì½”ë“œ
    ```
    
    </aside>
    
    <aside>
    ğŸ’¡ c. ì´ìƒê°’ ì²˜ë¦¬
    
    *ì´ìƒ ê°’ì€ ë¶„ìœ„ìˆ˜ ë“±ì„ í™œìš©í•˜ì—¬ ì²˜ë¦¬í•  ìˆ˜ ìˆì„ê²ƒ ê°™ë‹¤* 
    
    </aside>
    

1. **ë°ì´í„° ëª¨í˜• êµ¬ì¶•**
    
    modelì„ í•™ìŠµì‹œí‚¤ê¸° ìœ„í•´ ë¨¼ì € ëª¨ë¸ì„ êµ¬ì¶•í•œ í›„ ì „ì²˜ë¦¬ë¥¼ ì§„í–‰í•œ ë°ì´í„°ë¡œ fit í•¨ìœ¼ë¡œì¨ í•™ìŠµì„ ì§„í–‰í•œë‹¤.
    
    ëª¨ë¸ì€ task ì¢…ë¥˜ì— ë”°ë¼ íšŒê·€ëª¨ë¸, ë¶„ë¥˜ëª¨ë¸ë¡œ êµ¬ë¶„í•œë‹¤.
    
    **íšŒê·€ëª¨ë¸** 
    
    ì„ í˜•íšŒê·€(LinearRegression), 
    
    **ë¶„ë¥˜ëª¨ë¸**
    
    ëœë¤í¬ë ˆìŠ¤íŠ¸(RandomForestClassifier)
    
    ```python
    # Random Forest classifier
    **from sklearn.ensemble import RandomForestClassifier**
    
    # basic version
    model_rf = RandomForestClassifier()
    model_rf.fit(X_train, y_train.values.ravel())
    
    # feature importance ì¡°íšŒ ê°€ëŠ¥
    feature_impt = model_rf.feature_importances_
    ```
    
    ```python
    # XGBoost classifier
    **from xgboost import XGBClassifier**
    
    # basic version
    model_xgb1 = XGBClassifier() 
    model_xgb1.fit(X_train, Y_train.value.ravel())
    
    # custom version
    model_xgb2 = XGBClassifier(n_estimators=1000, learning_rate=0.1, max_depth=10)
    model_xg2.fit(X_train, Y_train.values.ravel(), **early_stopping_rounds=50,** \
    							**eval_metric='auc', eval_set=[(X_valid, Y_valid)]**, verbose=10)
    # í•™ìŠµì‹œ validation set ì‚¬ìš© ê°€ëŠ¥
    # eval_metric ì ìš© ê°€ëŠ¥
    # feature importance ì¡°íšŒ ê°€ëŠ¥
    feature_impt = model_xgb2.feature_importances_
    ```
    
    ```python
    # SVM classifier
    ```
    
    ```python
    # lightgbm
    
    ```
    

1. **ëª¨ë¸ íŠœë‹**
    1. **íŠ¹ì§• ì¤‘ìš”ë„** í™•ì¸ì„ í†µí•´ **íŠ¹ì§• ì‚­ì œ**
        
        ** í•™ìŠµëœ ëª¨ë¸ì„ í†µí•´ íŠ¹ì§•ì¤‘ìš”ë„(feature importance)ë¥¼ í™•ì¸ ê°€ëŠ¥í•œ ëª¨ë¸ë“¤ì´ ìˆìœ¼ë©°, ì´ë¥¼ í†µí•´ ì¤‘ìš”ë„ê°€ ë‚®ì€ íŠ¹ì§•ì„ ì‚­ì œí•´ì£¼ëŠ” ë“± ëª¨ë¸ ìµœì í™”ì— í™œìš© ê°€ëŠ¥í•¨.*
        
        [https://ahnty0122.tistory.com/51](https://ahnty0122.tistory.com/51)
        
        ![image](https://user-images.githubusercontent.com/86610517/181142202-809bc47e-7a11-4a8f-85a5-cf69e38bae23.png)
        
        ```python
        # X_trainê³¼ y_trainì´ ê°ê° ë‚˜ì™”ì„ ë•Œ joinì„ í†µí•´ íŠ¹ì§• ì¤‘ìš”ë„ í™•ì¸
        df.join(other.set_index('key'), on='key')
        ```
        
        ![image](https://user-images.githubusercontent.com/86610517/181142216-c9646787-31db-4db3-9345-4ceae4fe2b2b.png)
        
        ì½”ë“œ ì‹¤í–‰ í›„
        
        ```python
        df[df.columns[1:]].corr()['target'].sort_values(ascending=False)
        '''
        Output:
        Chance of Admit      1.000000
        CGPA                 0.869994
        GRE Score            0.799788
        TOEFL Score          0.789996
        University Rating    0.690693
        SOP                  0.688587
        LOR                  0.647938
        Research             0.574697
        Name: Chance of Admit , dtype: float64
        '''
        ```
        
    2. ~~ëª¨ë¸ì˜ **í•˜ì´í¼íŒŒë¼ë¯¸í„°** íŠœë‹~~
        
        *forë¬¸ì„ í†µí•´ í›„ë³´ íŒŒë¼ë¯¸í„°ë“¤ì„ ì„¤ì •í•˜ì—¬ best modelì„ ì°¾ì„ìˆ˜ë„ ìˆìŒ
        
        ```python
        **# ì‹¬í™”**
        from sklearn.model_selection import GridSearchCV
        #page 252
        
        ```
        
    3. 
2. **ë°ì´í„° ëª¨í˜• í‰ê°€**
    
    í‰ê°€í˜• ë°ì´í„°ë¡œ ì˜ˆì¸¡ê°’ì„ êµ¬í•œë‹¤.
    
    - predict í•¨ìˆ˜ë¥¼ ì‚¬ìš©í•˜ë©´ thresholdê°€ 0.5ì¸ ìƒíƒœë¡œ ì˜ˆì¸¡ê°’ì„ êµ¬í•˜ê²Œ ëœë‹¤.
    - predict_proba í•¨ìˆ˜ë¥¼ ì‚¬ìš©í•˜ë©´ ì˜ˆì¸¡ í™•ë¥ ì„ êµ¬í•  ìˆ˜ ìˆë‹¤.
    
    **ë¶„ë¥˜ ëª¨í˜• í‰ê°€**
    
    ```python
    # classification_report í•¨ìˆ˜ë¥¼ ì‚¬ìš©í•˜ë©´ í‰ê°€ì§€í‘œë¥¼ í•œë²ˆì— ë‹¤ êµ¬í•  ìˆ˜ ìˆì–´ì„œ ì¢‹ë‹¤.
    **from sklearn.metrics import classification_report**
    
    Y_pred = model.predict(X_test_preprocessed)
    print(classification_report(Y_test, Y_pred, labels=[0, 1])
    '''
    **output:**
    							presision, recall, f1-score, support
    label 1
    label 2
    label ..
    ..
    
    accuracy
    macro avg
    weighted avg
    '''
    
    # AUROC ê°’ì„ êµ¬í•˜ê¸° ìœ„í•´ì„œëŠ” predict_proba í•¨ìˆ˜ë¥¼ ì‚¬ìš©í•´ì•¼í•œë‹¤.
    from sklearn.metrics import roc_auc_score
    
    # ì¶”ì¶œí•´ì•¼í•˜ëŠ” í™•ë¥ ê°’ì´ ë‘ë²ˆì§¸ ì—´ì— ìˆì„ ë•Œ [:, 1]ì„ ë¶™ì—¬ì£¼ì–´ì•¼í•œë‹¤.
    Y_pred_prob = model.predict_proba(X_test_preprocessed)[:, 1]
    print(roc_auc_score(Y_test, Y_pred_prob) 
    ```
    
    **íšŒê·€ ëª¨í˜• í‰ê°€**
    
    ```python
    **from sklearn.metrics import mean_squared_error, r2_score**
    
    mse = mean_squared_error(Y_valid, Y_valid_pred)
    rmse = mean_squared_error(Y_valid, Y_valid_pred, squared=False)
    
    ```
    
    **ì €ì¥ ì½”ë“œ**
    
    ```python
    pd.DataFrame({'id': X_test['Serial No.'], 'target': pred}).to_csv('003000000.csv', index=False)
    ```

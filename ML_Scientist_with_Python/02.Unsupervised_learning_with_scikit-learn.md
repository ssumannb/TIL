**Unsupervised learning**

: finds patterns in data

- clustering : machine learning method that cluster the some data which have similar features
    - ex. KMeans, DBSCAN, Hierarchical clustering, Spectral Clustering, …
- dimension reduction : machine learning method that summarize dataset using its common occuring patterns
    - ex. PCA, SVD, …

**content list**

### 1. Clustering

- **k-means clustering :** `sklearn.cluster.KMeans`
    
    : finds clusters of samples
    
    number of clusters must be specified (n_clusters)
    
    `KMeans.cluster_centers_` : return the centroids of the clusters
    
    **Cluster labels for new samples**
    
    - New samples can be assigned to existing clusters
    - k-means remembers the mean of each cluster(the “centroid”)
    - Finds the nearest centroid to each new sample
    
    **Scatter plots** `matplotlib.pyplot` as plt
    
    - each point represents an iris sample
    - color points by cluter labels
    - using pyplot (matplotlib.pyplot)
- **Evaluating a clustering :** `pandas.crosstab` , `sklearn.cluster.KMeans.ineria_` , `sklearn.cluster.KMeans.labels_`
    - **can check correspondence with dataset label**
        - Informs choice of how many clusters to look for
        - Do the clusters correspond to the species? = check with **cross tabulation** (with pandas)
            
            ```python
            import pandas as pd
            
            df = pd.DataFrame({'labels' : label, 'species': species})
            print(df)
            
            ct = pd.crosstab(df['labels'], df['species'])
            print(ct)
            ```
            
            ![image](https://user-images.githubusercontent.com/86610517/181451423-125e92e3-c933-4a05-9504-265236e1dfc0.png)
            
    - **measuring clustering quality using only samples and their cluster labels** `sklearn.cluster.KMeans.ineria_`
        - 체크할 label이 없다면 **measuring quality of a clustering**
        - a good clustering has tight clusters
        - samples in each cluster bunched together
        
        → measures how spread out the clutesrs are (lower is better)
        
        ⇒ **Distnace** from each sample to centroid of its cluster 
        
        an attribute of **inertia** is the density of cluster as distance summation of each of the clusters
        
        ```python
        from sklearn.cluster import KMeans
        
        model = KMeans(n_cluster=3)
        model.fit(sample)
        print(model_intertia_)
        #> 78.9408...
        ```
        
        - k-means attemps to minimize the inertia when choosing cluster
        - **How many clusters to choose?**
            - a good clustering has tight clusters (so low **inertia**)
            - **but not to many clusters!**
            - choose an “elbow” in the inertia plot ⇒ “elbow” is where inertial begins to decrease more slowly
                
                ![image](https://user-images.githubusercontent.com/86610517/181451492-250238d1-61f2-4623-9f39-3ab74453182d.png)
                
    - **labels that the KMeans model predict** : `sklearn.cluster.KMeans.labels_`
- **Transforming freatures for better clustering :** `sklearn.preprocessing.StandardScaler`, `sklearn.preprocessing.Normalizer`
    - **Feature variances**
        - in k-means : feature variance = feature influence
            
            ⇒ overcome using stanndard scaler (others = MaxAbsScaler, Normalizer, …)
            
            use sklearn pipeline scaler → model fitting
            
            ```python
            from sklearn.preprocessing import StandardScaler
            from sklearn.cluster import KMeans
            from sklearn.pipeline import make_pipeline
            
            scaler = StandardScaler()
            kmeans = KMeans(n_cluster=3)
            
            pipeline = make_pipeline(scaler, kmeans)
            pipeline.fit(samples)
            
            label = pipeline.predict(new_samples)
            ```
            
            <aside>
            📌 Note that `Normalizer()` is different to `StandardScaler()`, which you used in the previous exercise. While `StandardScaler()` standardizes **features** (such as the features of the fish data from the previous exercise) by removing the mean and scaling to unit variance, `Normalizer()` rescales **each sample**
             - here, each company's stock price - independently of the other.
            
            </aside>
            

### 2. Visualization with hierarchical clustering and t-SNE

: Visualizations communicate insight

- **Hierarchical clustering analysis (HCA) - agglomerative HCA** : `scipy.cluster.hierarchy.linkage` , `scipy.cluster.hierarchy.dendrogram` , `sklearn.cluster.AgglomerativeClustering`
    
    : clusters the data as a **“bottom-up”** method
    
    - A hierarchy of groups, and Clusters are contained in one another
    - **dendrogram** groups the groups to larger groups *(to draw the dendrogram, have to assign the label name)*
        
        ![image](https://user-images.githubusercontent.com/86610517/181451572-37fd53bd-71b3-4733-b94b-f410082c415c.png)
        dendrogram
        
    1. every cluster begins in a seperate cluster
    2. at each step, the two closet clusters are merged
    3. contiue until all clusters in a single cluster
        
        ```python
        # with scipy library
        import matplotlib.pyplot as plt
        from scipy.cluster.hierarchy import linkage, dendrogram
        
        mergings = linkage(samples, method='complete')
        dendrogram(mergings, 
        						labels=country_names,
        						leaf_rotation=90
        						leaf_font_size=6)
        plt.show()
        
        # with sklearn library
        from sklearn.cluster import AgglomerativeClustering
        
        cluster = AgglomerativeClustering(n_clusters=3, affinity='euclidean', linkage='ward')
        
        cluster.fit(samples)
        lables = cluster.labels_
        ```
        
- **Hierarchical clustering analysis (HCA) - Divisive HCA**
    
    : clusters the data as a **“top-down”** method
    
    **divisive HCA is not used much in solving real-world problems*
    
- **Cluster labels in hierarchical clustering :** `sklearn.cluster.hierarchy.fcluster`
    
    HCA is not only a visualization tool! **Cluster labels at any intermediate stage can be recovered**
    
    - **Intermediate clusterings & height on dendrogram**
        
        : Height on dendrogram = distance between merging clusters
        
        Height on dendrogram specifies maximum distance between merging clusters
        
        ![image](https://user-images.githubusercontent.com/86610517/181451656-338fa72e-5f17-42ee-8c4d-64a32b51a632.png)
        **0.0 ~ 20.0 is the range of distance** 
        
    - **Distance between clusters**
        
        Defined by a “linkage method”. This specified via method parameter, (`linkage(samples, method=’complete’)`)
        
        ![image](https://user-images.githubusercontent.com/86610517/181451723-e2ae8c89-5eed-44e7-977e-76de7f1ebef4.png)
        단일(최단) 연결 / 완전(최장) 연결 / 중심 연결 / 평균 연결
        
        ```python
        import scipy.cluster.hierarchy import linkage
        import scipy.cluster.hierarchy import fcluster
        
        mergings = linkage(samples, method='complete')
        labels = fcluster(mergings, 15, criterion='distance')
        #> labels formatted numpy array
        
        # Aligning cluster labels with country names
        import pandas as pd
        pairs = pd.DataFrame({'labels': labels, 'countries': country_names})
        
        print(pairs.sort_values('labels'))
        ```
        
        **the cluster label of scipy starts from 1 (in sklearn, label starts from 0)*
        
        [https://docs.scipy.org/doc/scipy/reference/generated/scipy.cluster.hierarchy.fcluster.html](https://docs.scipy.org/doc/scipy/reference/generated/scipy.cluster.hierarchy.fcluster.html)
        
        > about the parameter “crierion” 
        : flat cluster를 형성하기 위한 기준을 설정하는 파라미터
        - inconsistent 
        - distance 
        - maxclust 
        - monocrit
        - maxclust_monocrit
        > 
        
- **t-SNE :** `sklearn.manifold.TNSE`
    
    : t-distributed stochastic neighbor embedding
    
    Maps samples to 2D space (or 3D)
    
    Map approximately preserves nearness of samples
    
    ```python
    # 2D numpy array samples : **samples**
    # list species giving species of labels as number (0, 1, or 2) : **species**
    import matplotlib.pyplot as plt
    from sklearn.manifold import TNSE
    
    model = TNSE(learning_rate=100)
    transformed = model.fit_transform(samples)
    xs = transformed[:, 0]
    xy = transformed[:, 1]
    
    plt.scatter(xs, xy, c=species)
    plt.show()
    ```
    
    **The characteristics of t-SNE**
    
    - has only fit_transform : 이는 새로운 데이터를 추가하지 못함을 의미한다.
    - t-SNE learning rate : 잘못된 learning rate 선택은 점들이 서로 뭉치게 만든다. (50~200 사이의 숫자를 try)
    - different every time

### 3. Decorrelating your data and dimension reduction

Dimension reduction summarizes a dataset using its common occuring patterns. In this chapter, you'll learn about the most fundamental of dimension reduction techniques, "Principal Component Analysis" ("PCA"). PCA is often used before supervised learning to improve model performance and generalization. It can also be useful for unsupervised learning. For example, you'll employ a variant of PCA will allow you to cluster Wikipedia articles by their content!

**Dimension reduction**

- More efficient storage and computation
- Remove less-informative ‘noise’ features which cause problems for prediction task
- **Principal Component Analysis (PCA) :** `sklearn.decomposition.PCA`
    
    : fundamental dimension reduction technique
    
    **steps** 
    
    1. ‘decorrelation’ of the data (not change a dimension of the data)
    2. reduce dimension
    
    **PCA aligns data with axes** 
    
    - Rotates data samples to be aligned with axes
    - Shift data samples to they have mean 0
    - No information lost
    
    ![image](https://user-images.githubusercontent.com/86610517/181451795-c16926c5-36d7-4694-9b75-6e17e674abd1.png) 
    shift samples
    
    ```python
    from sklearn.decomposition import PCA
    
    model = PCA()
    model.fit(sample)
    transformed = model.transform(samples) => PCA features
    mean_coordinates = model.mean_ => returing the coordinates of the mean of data
    ```
    
    > ***PCA features***
    > 
    > 
    > *Rows of transformed correspond to samples*
    > 
    > *Columns of transformed are the PCA features*
    > 
    > *Row gives PCA feature values of corresponding sample*
    > 
    
    ⇒ Resulting PCA features are not linearly correlated (”decorrelation”)
    
    **Principal components :** `sklearn.decomposition.PCA.components_`
    
    - “Principal component” = directions of variance
    - PCA aligns pricipal components with the axes
        
        ![image](https://user-images.githubusercontent.com/86610517/181451841-181ff1e8-d9a2-4cc2-95cc-eab40b5b0c10.png)
    
- **Intrinsic dimension :** `sklearn.decomposition.PCA.explained_variance_`
    
    : number of features needed to approximate the dataset
    
    Essential idea behind dimension reduction
    
    what is the most compact representation of the samples?
    
    can be detected by PCA
    
    > **Example**
    > 
    > 
    > Dataset with 2 features(longitude and latitude at a point along a flight path) → It can approximate using one feature = displacement(변위) along flight path
    > 
    > ⇒ **Is intrinsically 1-dimensionally** 
    > 
    
    **PCA identifies intrinsic dimension**
    
     **scatter plots은 2~3개의 feature를 가진 데이터셋에 대해서만 intrinsic feature를 확인할 수 있음 ⇒ **using PCA!***
    
    - PCA identifies intrinsic dimension when samples have any number of features
    - **Intrinsic dimension** = number of PCA features with **significant variance**
    - PCA features are ordered by variance descending
        
        ***Intrinsic dimension can be ambiguous and an idealization*
        
    
    ![image](https://user-images.githubusercontent.com/86610517/181451887-b5cd49b0-91a4-423f-bae1-f23cf9f01c6d.png)
    variance orders of features
    
    ```python
    import matplotlib.pyplot as plt
    from sklearn.decomposition import PCA
    
    pca = PCA()
    pca.fit(samples) 
    
    features = range(pca.n_components_)
    plt.bar(features, pca.explained_variance_)
    plt.xtick(features)
    plt.ylabel('variance')
    plt.xlabel('PCA feature') 
    plt.show()
    ```
    
- **Dimension reduction with PCA :** `sklearn.decomposition.PCA(n_components=@)` , `sklearn.decomposition.TruncateSVD`
    - PCA features are in decreasing order of variance
    - Assume the low variance features are ‘noise’ and high variance features are informative
    
    ```python
    from sklearn.decomposition import PCA
    
    pca = PCA(n_components=2) 
    pca.fit(samples)
    
    transformed = pca.transform(samples)
    print(transformed.shape) 
    > (150, 2)
    ```
    
    **In case of Word frequency arrays**
    
    - Data
        - Rows represent documents, columns represent words
        - Entries measure presence of each word in each document (measuring using ‘tf-idf’)
        
        ![image](https://user-images.githubusercontent.com/86610517/181451931-0cb6a83d-d5e1-42a4-b735-d97c9a3ddd6d.png)
        
    - Sparse arrays and csr_matrix
        - Sparse : most entries are zero
        - Can use `scipy.sparse.csr_matrix` instead of Numpy array
        - csr_matrix remebers only the non-zero entries (save space!)
        
        **scikit-learn PCA doesn’t support csr_matrix (documents data have csr_matrix format)*
        
        ⇒ using scikit-learn TruncatedSVD instead
        
        ```python
        # performs same transformation
        from sklearn.decomposition import TruncatedSVD
        
        model = TruncatedSVD(n_component=3)
        model.fit(documents)
        transformed = model.transformed(documents)
        ```
        
        ```python
        '''
        In this exercise, you'll create a tf-idf word frequency array 
        for a toy collection of documents.
        For this, use the TfidfVectorizer from sklearn. 
        It transforms a list of documents into a word frequency array, 
        which it outputs as a csr_matrix. 
        It has fit() and transform() methods like other sklearn objects.
        You are given a list documents of toy documents about pets. 
        Its contents have been printed in the IPython Shell.
        '''
        # Import TfidfVectorizer
        from sklearn.feature_extraction.text import TfidfVectorizer
        
        # Create a TfidfVectorizer: tfidf
        tfidf = TfidfVectorizer()
        
        # Apply fit_transform to document: csr_mat
        csr_mat = tfidf.fit_transform(documents)
        
        # Print result of toarray() method
        print(csr_mat.toarray())
        
        # Get the words: words
        words = tfidf.get_feature_names()
        
        # Print words
        print(words)
        ```
